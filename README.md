# Raga â€” Retrieval Augmented Generation (RAG) Chatbot ğŸš€

Raga is a **document-aware AI chatbot** that allows users to upload PDFs and ask questions strictly based on the uploaded content.  
It is built using **Flask**, **LangChain**, **FAISS**, and **Hugging Face Inference APIs**, containerized with **Docker**, and deployed on **AWS EC2**.

The project focuses on **correct system design**, **session isolation**, and **real-world deployment challenges**, rather than just model usage.

---

## âœ¨ Features

- ğŸ“„ Upload PDF documents
- ğŸ¤– Ask questions based only on document content (RAG)
- ğŸ§  Semantic search using embeddings + FAISS
- ğŸ”’ Per-user session isolation (no cross-device leakage)
- â™»ï¸ Fresh sessions & documents on app restart (demo-safe)
- ğŸ³ Fully Dockerized
- â˜ï¸ Deployed on AWS EC2 with Elastic IP
- ğŸ“œ Clean logging and error handling
- ğŸ–¥ï¸ Minimal and user-friendly UI

---

## ğŸ—ï¸ Project Architecture

```

Raga/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main/               # UI & chat routes
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â”œâ”€â”€ rag/                # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ loaders.py      # PDF loading & chunking
â”‚   â”‚   â”œâ”€â”€ retriever.py    # FAISS vector store
â”‚   â”‚   â”œâ”€â”€ generator.py   # LLM response generation
â”‚   â”‚   â””â”€â”€ pipeline.py    # End-to-end RAG flow
â”‚   â”œâ”€â”€ static/             # CSS
â”‚   â”œâ”€â”€ templates/          # HTML
â”‚   â””â”€â”€ **init**.py         # App factory & session config
â”‚
â”œâ”€â”€ uploads/                # Session-scoped uploads
â”œâ”€â”€ run.py                  # App entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env                    # HF_API_TOKEN
â””â”€â”€ README.md

````

---

## ğŸ§  How RAG Works in This Project

1. User uploads a PDF
2. Text is extracted and chunked
3. Chunks are embedded using Sentence Transformers
4. FAISS performs similarity search
5. Relevant context is sent to an LLM
6. LLM generates an answer **only from document context**

If the answer is not present in the document, the model explicitly says so.

---

## ğŸ” Session Handling (Important)

- Server-side sessions using **Flask-Session**
- Each user gets:
  - A unique session
  - A unique upload directory
- Sessions and uploads are **reset on app restart**
- Prevents:
  - Cross-user document leakage
  - Stale demo data
  - Cookie size issues

---

## ğŸš€ Running Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/Raga.git
cd Raga
````

### 2ï¸âƒ£ Create `.env`

```env
HF_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

### 3ï¸âƒ£ Run with Docker

```bash
docker build -t student-ai-rag .
docker run -d \
  --name student-ai-rag \
  -p 5000:5000 \
  --env-file .env \
  student-ai-rag
```

Open:

```
http://localhost:5000
```

---

## â˜ï¸ Deployment (AWS EC2)

* EC2 Ubuntu instance
* Docker installed
* Elastic IP attached (stable URL)
* App exposed on port `5000`

---
## ğŸ“Œ Tech Stack

* **Backend**: Flask
* **RAG**: LangChain, FAISS
* **Embeddings**: Sentence Transformers
* **LLM**: Hugging Face Inference API
* **Sessions**: Flask-Session
* **Deployment**: Docker, AWS EC2
* **OS**: Linux (Ubuntu)

## ğŸ‘¨â€ğŸ’» Author

**Ankit Yadav**
Built with â¤ï¸ while learning real-world ML systems and deployment.

---
